{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "844f421d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8704f88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU chonkie datetime ipykernel jinja2 matplotlib networkx numpy openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f84d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"datasets<3.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ffd7c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['company', 'date', 'transcript'],\n",
       "    num_rows: 150\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "hf_dataset_name = \"jlh-ibm/earnings_call\"\n",
    "subset_options = [\"stock_prices\", \"transcript-sentiment\", \"transcripts\"]\n",
    "\n",
    "hf_dataset = load_dataset(hf_dataset_name, subset_options[2])\n",
    "my_dataset = hf_dataset[\"train\"]\n",
    "\n",
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3194e845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('AMD',\n",
       " datetime.date(2016, 7, 21),\n",
       " '\\n\\nThomson Reuters StreetEvents Event Transcript\\nE D I T E D   V E R S I O N\\n\\nQ2 2016 Advanced Micro Devices Inc Earnings Call\\nJULY 21, 2016 / 9:00PM GMT\\n\\n==============================================')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = my_dataset[0]\n",
    "row[\"company\"], row[\"date\"], row[\"transcript\"][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb68e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'AMZN': 18,\n",
       "         'AMD': 17,\n",
       "         'INTC': 16,\n",
       "         'NVDA': 16,\n",
       "         'MU': 15,\n",
       "         'ASML': 15,\n",
       "         'AAPL': 14,\n",
       "         'GOOGL': 14,\n",
       "         'CSCO': 13,\n",
       "         'MSFT': 12})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "company_counts = Counter(my_dataset[\"company\"])\n",
    "company_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "295b5a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: db_interface in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install db_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f2dbc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed existing database: cookbook.db\n",
      "Connected to SQLite database: cookbook.db\n"
     ]
    }
   ],
   "source": [
    "from db_interface import make_connection\n",
    "\n",
    "\n",
    "sqlite_conn = make_connection(memory=False, refresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c08e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    id: uuid.UUID = Field(default_factory=uuid.uuid4)\n",
    "    text: str\n",
    "    metadata: dict[str, Any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8819417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from pydantic import field_validator\n",
    "\n",
    "class Transcript(BaseModel):\n",
    "\n",
    "    id: uuid.UUID = Field(default_factory=uuid.uuid4)\n",
    "    text: str\n",
    "    company: str\n",
    "    date: datetime\n",
    "    quarter: str | None = None\n",
    "    chunks: list[Chunk] | None = None\n",
    "\n",
    "    @field_validator(\"date\", mode=\"before\")\n",
    "    @classmethod\n",
    "    def to_datetime(cls, d: Any) -> datetime:\n",
    "        if isinstance(d, str):\n",
    "            return d\n",
    "        if hasattr(d, \"isoformat\"):\n",
    "            return datetime.fromisoformat(d.isoformat())\n",
    "        return datetime.fromisoformat(str(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47d9967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# This will prompt you to enter the key securely\n",
    "# api_key = getpass(\"Enter your OpenAI API key: \")\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24202dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from typing import Any\n",
    "\n",
    "# from chonkie import OpenAIEmbeddings, SemanticChunker\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# class Chunker:\n",
    "#     def __init__(self, model: str = \"text-embedding-3-small\"):\n",
    "#         self.model = model\n",
    "    \n",
    "#     def find_quarter(self, text: str) -> str | None:\n",
    "#         search_results = re.findall(r\"[Q]\\d\\s\\d{4}\", text)\n",
    "\n",
    "#         if search_results:\n",
    "#             quarter = str(search_results[0])\n",
    "#             return quarter\n",
    "        \n",
    "#         return None\n",
    "    \n",
    "#     def generate_transcripts_and_chunks(\n",
    "#         self,\n",
    "#         dataset: Any,\n",
    "#         company: list[str] | None = None,\n",
    "#         text_key: str = \"transcript\",\n",
    "#         company_key: str = \"company\",\n",
    "#         date_key: str = \"date\",\n",
    "#         threshold_value: float = 0.7,\n",
    "#         min_sentences: int = 3,\n",
    "#         num_workers: int = 50,\n",
    "#     ) -> list[Transcript]:\n",
    "#         transcripts = [\n",
    "#             Transcript(\n",
    "#                 text=d[text_key],\n",
    "#                 company=d[company_key],\n",
    "#                 date=d[date_key],\n",
    "#                 quarter=self.find_quarter(d[text_key]),\n",
    "#             )\n",
    "#             for d in dataset\n",
    "#         ]\n",
    "\n",
    "#         if company:\n",
    "#             transcripts = [t for t in transcripts if t.company in company]\n",
    "\n",
    "#         def _process(t: Transcript) -> Transcript:\n",
    "#             if not hasattr(_process, \"chunker\"):\n",
    "#                 embed_model = OpenAIEmbeddings(self.model)\n",
    "#                 _process.chunker = SemanticChunker(\n",
    "#                     embedding_model=embed_model,\n",
    "#                     threshold=threshold_value,\n",
    "#                     min_sentences=max(min_sentences, 1),\n",
    "#                 )\n",
    "#             semantic_chunks = _process.chunker.chunk(t.text)\n",
    "#             t.chunks = [\n",
    "#                 Chunk(\n",
    "#                     text=c.text,\n",
    "#                     metadata={\n",
    "#                         \"start_index\": getattr(c, \"start_index\", None),\n",
    "#                         \"end_index\": getattr(c, \"end_index\", None),\n",
    "#                     },\n",
    "#                 )\n",
    "#                 for c in semantic_chunks\n",
    "#             ]\n",
    "\n",
    "#             return t\n",
    "        \n",
    "#         with ThreadPoolExecutor(max_workers=num_workers) as pool:\n",
    "#             futures = [pool.submit(_process, t) for t in transcripts]\n",
    "#             transcripts = [\n",
    "#                 f.result()\n",
    "#                 for f in tqdm(\n",
    "#                     as_completed(futures),\n",
    "#                     total=len(futures),\n",
    "#                     desc=\"Generating Semantic Chunks\"\n",
    "#                 )\n",
    "#             ]\n",
    "\n",
    "#         return transcripts\n",
    "    \n",
    "# raw_data = list(my_dataset)\n",
    "\n",
    "# chunker = Chunker()\n",
    "# transcripts = chunker.generate_transcripts_and_chunks(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bde187f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Semantic Chunks with Gemini: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 581465.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully processed 150 transcripts with Gemini!\n",
      "\n",
      "ðŸ“„ Transcript 1: AAPL (Q3 2016)\n",
      "   Chunks: 94\n",
      "   First chunk: Thomson Reuters StreetEvents Event Brief\n",
      "E D I T E D   V E R S I O N\n",
      "\n",
      "Q3 2016 Apple Inc Earnings Cal...\n",
      "\n",
      "ðŸ“„ Transcript 2: CSCO (Q3 2016)\n",
      "   Chunks: 108\n",
      "   First chunk: Thomson Reuters StreetEvents Event Transcript\n",
      "E D I T E D   V E R S I O N\n",
      "\n",
      "Q3 2016 Cisco Systems Inc...\n",
      "\n",
      "ðŸ“„ Transcript 3: AMZN (Q2 2019)\n",
      "   Chunks: 73\n",
      "   First chunk: Thomson Reuters StreetEvents Event Transcript\n",
      "E D I T E D   V E R S I O N\n",
      "\n",
      "Q2 2019 Amazon com Inc Ea...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the working Gemini chunker from the external file\n",
    "from gemini_chunker import GeminiChunker\n",
    "\n",
    "# Set your Gemini API key\n",
    "\n",
    "\n",
    "# Initialize the chunker\n",
    "chunker = GeminiChunker()\n",
    "\n",
    "# Process your data\n",
    "raw_data = list(my_dataset)\n",
    "transcripts = chunker.generate_transcripts_and_chunks(raw_data)\n",
    "\n",
    "print(f\"âœ… Successfully processed {len(transcripts)} transcripts with Gemini!\")\n",
    "\n",
    "# Display some results\n",
    "for i, transcript in enumerate(transcripts[:3]):  # Show first 3\n",
    "    print(f\"\\nðŸ“„ Transcript {i+1}: {transcript.company} ({transcript.quarter})\")\n",
    "    print(f\"   Chunks: {len(transcript.chunks) if transcript.chunks else 0}\")\n",
    "    if transcript.chunks:\n",
    "        print(f\"   First chunk: {transcript.chunks[0].text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32576403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_model import LABEL_DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27dea09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class TemporalType(Enum):\n",
    "    \n",
    "    ATEMPORAL = \"ATEMPORAL\"\n",
    "    STATIC = \"STATIC\"\n",
    "    DYNAMIC = \"DYNAMIC\"\n",
    "\n",
    "class StatementType(Enum):\n",
    "    FACT = \"FACT\"\n",
    "    OPINION = \"OPINION\"\n",
    "    PREDICTION = \"PREDICTION\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17b05a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import field_validator\n",
    "\n",
    "class RawStatement(BaseModel):\n",
    "    statement: str\n",
    "    statement_type: StatementType\n",
    "    temporal_type: TemporalType\n",
    "\n",
    "    @field_validator(\"temporal_type\", mode=\"before\")\n",
    "    @classmethod\n",
    "    def _parse_temporal_label(cls, value: str | None) -> TemporalType:\n",
    "        if value is None:\n",
    "            return TemporalType.ATEMPORAL\n",
    "        cleaned_value = value.strip().upper()\n",
    "        try:\n",
    "            return TemporalType(cleaned_value)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Invalid Temporal Type: {value}\")\n",
    "        \n",
    "    @field_validator(\"statement_type\", mode=\"before\")\n",
    "    @classmethod\n",
    "    def _parse_statement_label(cls, value: str | None = None) -> StatementType:\n",
    "        if value is None:\n",
    "            return StatementType.FACT\n",
    "        cleaned_value = value.strip().upper()\n",
    "        try:\n",
    "            return StatementType(cleaned_value)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Invalid temporal type\")\n",
    "        \n",
    "class RawStatementList(BaseModel):\n",
    "    statements: list[RawStatement]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a660784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statement import statement_extraction_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef118980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
